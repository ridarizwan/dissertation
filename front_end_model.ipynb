{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7BhoiST0H0li",
        "outputId": "be9d8fbe-8224-4258-d223-d17b37db936f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scipy 1.13.1\n",
            "Uninstalling scipy-1.13.1:\n",
            "  Successfully uninstalled scipy-1.13.1\n",
            "Collecting scipy==1.10.0\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m437.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.10.0) (1.26.4)\n",
            "Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "Successfully installed scipy-1.10.0\n",
            "Collecting Gpy\n",
            "  Downloading GPy-1.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting GPyOpt\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from Gpy) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Gpy) (1.16.0)\n",
            "Collecting paramz>=0.9.6 (from Gpy)\n",
            "  Downloading paramz-0.9.6-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from Gpy) (3.0.11)\n",
            "Requirement already satisfied: scipy<=1.12.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from Gpy) (1.10.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.6->Gpy) (4.4.2)\n",
            "Downloading GPy-1.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paramz-0.9.6-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: GPyOpt\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83602 sha256=3c33316b4f1e5f66ea9382507efb5598fe1f25dffb707274eaf7df89f8be4846\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/17/52/9d818b4c60f733bf49d5cf82bc2758ebbdc57a0471137c37be\n",
            "Successfully built GPyOpt\n",
            "Installing collected packages: paramz, Gpy, GPyOpt\n",
            "Successfully installed GPyOpt-1.2.6 Gpy-1.13.2 paramz-0.9.6\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall scipy -y\n",
        "!pip install scipy==1.10.0\n",
        "!pip install Gpy GPyOpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qaQaB2-c6WeF",
        "outputId": "a3a4d1e7-4c1a-48e6-aa67-df0b1e924a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting tensorflow-probability==0.16.0\n",
            "  Downloading tensorflow_probability-0.16.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.11.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.16.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.16.0) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.16.0) (0.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.7)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.16.0-py2.py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, tensorflow-probability, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.64.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.23.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.23.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-probability-0.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "bfe85754a7e346a99727ac1fbc55150a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0 tensorflow-probability==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUEgYHsmH-h0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tXYOo4X55lGx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error, accuracy_score\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "import GPy\n",
        "import GPyOpt\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag-Qrd6G7P4J"
      },
      "source": [
        "Import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zAfwlBaIT-0",
        "outputId": "0334b93b-9f8a-4e2d-bdcc-a75264da8fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "date_model = tf.keras.models.load_model('/content/drive/MyDrive/my_bnn_model_savedmodel')\n",
        "\n",
        "\n",
        "# Load the scaler object from Google Drive\n",
        "scaler = joblib.load('/content/drive/MyDrive/scaler_amt.pkl')\n",
        "\n",
        "with open('/content/drive/MyDrive/gpr_model.pkl', 'rb') as file:\n",
        "    amt_model = pickle.load(file)\n",
        "\n",
        "\n",
        "label_path = '/content/drive/My Drive/le_merchant.pkl'\n",
        "\n",
        "# Load the LabelEncoder\n",
        "with open(label_path, 'rb') as file:\n",
        "    le_merchant = pickle.load(file)\n",
        "mer_model_path = '/content/drive/My Drive/best_model.pkl'\n",
        "\n",
        "# Load the model from the specified path\n",
        "mer_model = joblib.load(mer_model_path)\n",
        "\n",
        "# Define the path to the file in Google Drive\n",
        "file_path = '/content/drive/My Drive/kbins_discretizer.pkl'\n",
        "\n",
        "# Load the fitted KBinsDiscretizer\n",
        "kb_loaded = joblib.load(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILzyDDDk7ZPI"
      },
      "source": [
        "Data Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "byayxKadskMj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_transform(data):\n",
        "  data['P ORIGINAL GROSS AMT BIN'] = kb_loaded.transform(data[['ORIGINAL GROSS AMT']])\n",
        "\n",
        "\n",
        "  data = data.rename(columns={\"ORIGINAL GROSS AMT\": \"P ORIGINAL GROSS AMT\",\"MERCHANT NAME\":\"P MERCHANT NAME\"})\n",
        "  #le=LabelEncoder()\n",
        "\n",
        "  data['P MERCHANT NAME'] = le_merchant.fit_transform(data['P MERCHANT NAME'])\n",
        "\n",
        "  data['TRANS DATE'] = pd.to_datetime(data['TRANS DATE'], errors='coerce')\n",
        "\n",
        "  data['P_YEAR'] = data['TRANS DATE'].dt.year\n",
        "  data['P_MONTH'] = data['TRANS DATE'].dt.month\n",
        "  data['P_DAY'] = data['TRANS DATE'].dt.day\n",
        "  data['P_DAY_OF_WEEK'] = data['TRANS DATE'].dt.dayofweek\n",
        "\n",
        "  data_date=data#[['P_YEAR','P_MONTH','P_DAY','P_DAY_OF_WEEK','CARD NUMBER','P MERCHANT NAME','P ORIGINAL GROSS AMT' ]]\n",
        "  data_amt=data[['P_YEAR', 'P_MONTH', 'P_DAY', 'P_DAY_OF_WEEK', 'P MERCHANT NAME','P ORIGINAL GROSS AMT', 'CARD NUMBER']]#\n",
        "  data_mer=data[['CARD NUMBER',\t'P ORIGINAL GROSS AMT BIN'\t,'P_YEAR'\t,'P_MONTH'\t,'P_DAY','P MERCHANT NAME']]\n",
        "  return data_date,data_amt, data_mer\n",
        "#data_date,data_amt, data_mer=data_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7sW4yiR7mex"
      },
      "source": [
        "DATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pd3WqeOeA6EQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def date_pred(data):\n",
        "     data_date=data[['P_YEAR','P_MONTH','P_DAY','P_DAY_OF_WEEK','CARD NUMBER','P MERCHANT NAME','P ORIGINAL GROSS AMT' ]]\n",
        "     data_scaled = tf.convert_to_tensor(data_date, dtype=tf.float32)\n",
        "     y=date_model.predict(data_scaled)\n",
        "     y = np.argmax(y, axis=1)\n",
        "    # Add days from 'days_to_add' to the 'dates'\n",
        "     new = data['TRANS DATE'] + pd.to_timedelta(y, unit='days')\n",
        "\n",
        "     return new\n",
        "#date_pred(data_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAWr_p_Q7qWN"
      },
      "source": [
        "Amount model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WWz1WCWbaIQG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def amt_predict(data):\n",
        "# Use the scaler to transform new data\n",
        "  data_scaled = scaler.transform(data)\n",
        "  data_scaled\n",
        "\n",
        "  #scaler = StandardScaler()\n",
        "  #data_scaled = scaler.fit_transform(data_date)\n",
        "  y,var=amt_model.predict(data_scaled)\n",
        "  y=y.astype(float)\n",
        "  return y\n",
        "#amt_predict(data_amt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vtI90cv7yP4"
      },
      "source": [
        "Merchant Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AWDB3dam-x4Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming le_merchant was used to fit on the training data\n",
        "def find_closest_label(unseen_label, seen_labels):\n",
        "    # Find the closest label in the seen labels\n",
        "    closest_label = min(seen_labels, key=lambda x: abs(x - unseen_label))\n",
        "    return closest_label\n",
        "mer=[0,2,1,5,9,10,20,30,40,60,90,1000,2000,3000,40000]\n",
        "def label_decode(mer):\n",
        "\n",
        "      try:\n",
        "          demer = le_merchant.inverse_transform(mer)\n",
        "      except ValueError as e:\n",
        "          print(\"Error:\", e)\n",
        "\n",
        "          # Get the classes seen by the encoder\n",
        "          seen_labels = set(le_merchant.transform(le_merchant.classes_))\n",
        "\n",
        "          # Initialize an array to store decoded merchants\n",
        "          demer = []\n",
        "\n",
        "          for label in mer:\n",
        "              if label in seen_labels:\n",
        "                  demer.append(le_merchant.inverse_transform([label])[0])\n",
        "              else:\n",
        "                  # Find the closest label for the unseen label\n",
        "                  closest_label = find_closest_label(label, seen_labels)\n",
        "                  demer.append(le_merchant.inverse_transform([closest_label])[0])\n",
        "\n",
        "      # Convert demer to numpy array or keep as a list\n",
        "      demer = np.array(demer)\n",
        "      return demer\n",
        "#label_decode(mer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rra_IMkPLfqR"
      },
      "outputs": [],
      "source": [
        "def mer_predict(data):\n",
        "    decode_mer = []\n",
        "\n",
        "    # Loop through each row in the data\n",
        "    for i in range(len(data)):\n",
        "        try:\n",
        "            # Predict for the specific row of data\n",
        "            row_data = data.iloc[[i]]  # Select row as DataFrame\n",
        "            mer = mer_model.predict(row_data)  # Predict for the specific row\n",
        "\n",
        "            # Try decoding the predicted value\n",
        "            try:\n",
        "                transformed_value = label_decode(mer)[0]  # Assuming label_decode returns an array\n",
        "                decode_mer.append(transformed_value)\n",
        "            except :\n",
        "                # If there's an error in decoding, append 'others'\n",
        "                decode_mer.append('others')\n",
        "\n",
        "        except :\n",
        "            # If there's an error in predicting for that row, append 'others'\n",
        "            decode_mer.append('others')\n",
        "\n",
        "    return decode_mer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAb1FaQ29oP0"
      },
      "source": [
        "Predict transaction at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xnNs-1MKPx9Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def all_predictions(data,n):\n",
        "        updated_transactions=None\n",
        "        data_sorted = data.sort_values(by=['CARD NUMBER', 'TRANS DATE'], ascending=[True, False])\n",
        "        updated_transactions = data_sorted.groupby('CARD NUMBER').first().reset_index()\n",
        "        original_index = updated_transactions.index\n",
        "        for n in range(0,n):\n",
        "            updated_transactions['TRANS DATE'] = pd.to_datetime(updated_transactions['TRANS DATE'])\n",
        "\n",
        "            latest_transactions = updated_transactions.loc[updated_transactions.groupby('CARD NUMBER')['TRANS DATE'].idxmax()].reset_index(drop=True)\n",
        "            #latest_transactions = updated_transactions.groupby('CARD NUMBER').first().reset_index()\n",
        "\n",
        "            x_date, x_amt, x_mer = data_transform(latest_transactions)\n",
        "            # Generate predictions\n",
        "            date = date_pred(x_date)\n",
        "            amt = amt_predict(x_amt)\n",
        "            #mer = mer_predict(x_mer)\n",
        "            #if x_mer.shape[1] != mer_model.n_features_in_:\n",
        "            #   print(\"Error: Number of features in x_mer does not match mer_model.\")\n",
        "            # Handle the error, e.g., by resizing x_mer or retraining mer_model\n",
        "            #continue\n",
        "            mer = mer_predict(x_mer)\n",
        "\n",
        "            amt = amt.ravel()\n",
        "            # Create a new DataFrame with the predicted values\n",
        "            new_transaction = pd.DataFrame({\n",
        "                'CARD NUMBER': latest_transactions['CARD NUMBER'],  # Or any specific card number logic\n",
        "                'TRANS DATE': date,\n",
        "                'MERCHANT NAME': mer,\n",
        "                'ORIGINAL GROSS AMT': amt})\n",
        "\n",
        "            updated_transactions = pd.concat([updated_transactions, new_transaction], ignore_index=True)\n",
        "            #updated_transactions.append(new_transaction)\n",
        "        updated_transactions = updated_transactions.drop(index=original_index)\n",
        "        updated_transactions['ORIGINAL GROSS AMT']=updated_transactions['ORIGINAL GROSS AMT'].astype(float)\n",
        "        updated_transactions['ORIGINAL GROSS AMT']=updated_transactions['ORIGINAL GROSS AMT'].round(2)\n",
        "        return updated_transactions\n",
        "#all_predictions(df,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MizctkYR92xu"
      },
      "source": [
        "Prediction of single card number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OPrHhw3vu7D-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def card_predictions(data,n, card):\n",
        "        updated_transactions=None\n",
        "        updated_transactions = data.loc[data['CARD NUMBER'] ==card].sort_values('TRANS DATE', ascending=False).head(1).reset_index()\n",
        "\n",
        "        original_index = updated_transactions.index\n",
        "        for n in range(0,n):\n",
        "            new_transaction = None\n",
        "            updated_transactions['TRANS DATE'] = pd.to_datetime(updated_transactions['TRANS DATE'])\n",
        "\n",
        "            latest_transactions = updated_transactions.loc[updated_transactions['TRANS DATE'] == updated_transactions['TRANS DATE'].max()].head(1).reset_index(drop=True)\n",
        "            #latest_transactions = updated_transactions.groupby('CARD NUMBER').first().reset_index()\n",
        "            #print('lll',latest_transactions)\n",
        "            #print(updated_transactions)\n",
        "            x_date, x_amt, x_mer = data_transform(latest_transactions)\n",
        "            # Generate predictions\n",
        "            date = date_pred(x_date)\n",
        "            amt = amt_predict(x_amt)\n",
        "            #mer = mer_predict(x_mer)\n",
        "            #if x_mer.shape[1] != mer_model.n_features_in_:\n",
        "            #   print(\"Error: Number of features in x_mer does not match mer_model.\")\n",
        "            # Handle the error, e.g., by resizing x_mer or retraining mer_model\n",
        "            #continue\n",
        "            mer = mer_predict(x_mer)\n",
        "\n",
        "            amt = amt.ravel()\n",
        "            # Create a new DataFrame with the predicted values\n",
        "            new_transaction = pd.DataFrame({\n",
        "                'CARD NUMBER': latest_transactions['CARD NUMBER'],  # Or any specific card number logic\n",
        "                'TRANS DATE': date,\n",
        "                'MERCHANT NAME': mer,\n",
        "                'ORIGINAL GROSS AMT': amt})\n",
        "\n",
        "            updated_transactions = pd.concat([updated_transactions, new_transaction], ignore_index=True)\n",
        "            #updated_transactions.append(new_transaction)\n",
        "        updated_transactions = updated_transactions.drop(index=original_index)\n",
        "\n",
        "        return updated_transactions\n",
        "#card_predictions(df,1,8577)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjzZr_qI-FaS"
      },
      "source": [
        "# FRONT END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "saxtO9qF-Zoo",
        "outputId": "d36ab562-a316-4a6c-9577-f67c37072ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://bvc2nnslsiw-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-NtfmoZEnYC",
        "outputId": "19d10332-5792-43da-b4f4-e3655a3be21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 15:55:33] \"GET /?authuser=0 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 15:55:33] \"\u001b[33mGET /favicon.ico?authuser=0 HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded and stored in memory.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Error: y contains previously unseen labels: [1667]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Error: y contains previously unseen labels: [1667]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Error: y contains previously unseen labels: [1667]\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "Error: y contains previously unseen labels: [1667]\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 15:55:48] \"POST /?authuser=0 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: y contains previously unseen labels: [1667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [06/Sep/2024 15:55:49] \"\u001b[33mGET /favicon.ico?authuser=0 HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, render_template_string, send_file\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Global variables\n",
        "uploaded_file = None\n",
        "processed_data = None  # Initialize processed_data as None\n",
        "\n",
        "# HTML template with upload form, number input boxes, and calculate button\n",
        "upload_form = \"\"\"\n",
        "<!doctype html>\n",
        "<html lang=\"en\">\n",
        "  <head>\n",
        "    <title>CONSUMER SPENDING PREDICTOR</title>\n",
        "  </head>\n",
        "  <style>\n",
        "      body {\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        align-items: flex-start;   /* Align items to the left */\n",
        "        padding-left: 50px;    /* Add 50px padding on the left */\n",
        "        height: 100vh;\n",
        "        margin: 20px;\n",
        "      }\n",
        "      form {\n",
        "        text-align: left; /* Align form elements to the left */\n",
        "      }\n",
        "      input[type=\"submit\"] {\n",
        "      margin-top: 10px; /* less space */\n",
        "    }\n",
        "    </style>\n",
        "  <body>\n",
        "    <h1>Consumer Spending Predictor</h1>\n",
        "    <form method=\"POST\" enctype=\"multipart/form-data\">\n",
        "      <div>\n",
        "        <style>\n",
        "  .input-container {\n",
        "    display: block; /* Ensures each input container takes up the full width */\n",
        "    margin-bottom: 10px; /* Add spacing between input boxes */\n",
        "  }\n",
        "\n",
        "  .input-container label {\n",
        "    display: block; /* Ensures the label takes up the full width */\n",
        "    margin-bottom: 5px; /* Add spacing between label and input */\n",
        "  }\n",
        "\n",
        "  .input-container input[type=\"number\"] {\n",
        "    width: 200px; /* Make input boxes take up full width of their container */\n",
        "  }\n",
        "\n",
        "</style>\n",
        "\n",
        "<div class=\"input-container\">\n",
        "  <label for=\"number1\">Enter Number of Predictions:</label>\n",
        "  <input type=\"number\" id=\"number1\" name=\"number1\" required>\n",
        "</div>\n",
        "\n",
        "<div class=\"input-container\">\n",
        "  <label for=\"number2\">Enter Card Number:</label>\n",
        "  <input type=\"number\" id=\"number2\" name=\"number2\">\n",
        "</div>\n",
        "      <div>\n",
        "        <label for=\"file\">Upload a file:</label>\n",
        "        <input type=\"file\" name=\"file\">\n",
        "      </div>\n",
        "      <div>\n",
        "      <label for=\"wholeFilePrediction\">Future Transactions for Uploaded File:\n",
        "      <input type=\"submit\" id=\"wholeFilePrediction\" name=\"action\" value=\"Predict for Whole File\">\n",
        "\n",
        "      </div>\n",
        "      <div>\n",
        "      <label for=\"cardPrediction\">Future Transactions for Card Number:\n",
        "      <input type=\"submit\" id=\"cardPrediction\" name=\"action\" value=\"Predict for Card Number\">\n",
        "\n",
        "      </div>\n",
        "\n",
        "    </form>\n",
        "    {% if message %}\n",
        "    <p style=\"color: green;\">{{ message }}</p>\n",
        "    {% endif %}\n",
        "    {% if result is not none %}\n",
        "    {% endif %}\n",
        "    {% if table %}\n",
        "    <h2>{{ button_name }} Results</h2>  <!-- Display the button name as the header -->\n",
        "    {{ table|safe }}\n",
        "    <form action=\"/download\" method=\"GET\">\n",
        "      <button type=\"submit\">Save CSV</button> <!-- Save button appears after table is displayed -->\n",
        "    </form>\n",
        "    {% endif %}\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Dummy preprocess function to load data into a DataFrame\n",
        "def preprocess_data(file_stream):\n",
        "    try:\n",
        "        file_stream.seek(0)  # Go to the beginning of the stream\n",
        "        df = pd.read_csv(file_stream)\n",
        "    except Exception as e:\n",
        "        df = pd.DataFrame()  # If there's an issue, return an empty DataFrame\n",
        "        print(f\"Error processing file: {e}\")\n",
        "    return df\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def home():\n",
        "    global uploaded_file, processed_data\n",
        "    message = \"\"\n",
        "    table = None\n",
        "    button_name = None\n",
        "\n",
        "    if request.method == \"POST\":\n",
        "        number1 = request.form.get(\"number1\")\n",
        "        number2 = request.form.get(\"number2\")\n",
        "        action = request.form.get(\"action\")\n",
        "        file = request.files.get(\"file\")\n",
        "\n",
        "        if file and file.filename != '':\n",
        "            uploaded_file = io.BytesIO(file.read())\n",
        "            message = \"File uploaded successfully!\"\n",
        "            print(f\"File uploaded and stored in memory.\")\n",
        "\n",
        "        if uploaded_file:\n",
        "            data = preprocess_data(uploaded_file)\n",
        "            if not data.empty:\n",
        "                table = data.to_html()\n",
        "            else:\n",
        "                message = \"The uploaded file appears to be empty or invalid.\"\n",
        "\n",
        "        if action == \"Predict for Whole File\":\n",
        "            try:\n",
        "                num1 = int(number1)\n",
        "                if uploaded_file:\n",
        "                    df = preprocess_data(uploaded_file)\n",
        "                    if not df.empty:\n",
        "                        data = all_predictions(df, num1)\n",
        "                        table = data[['TRANS DATE','CARD NUMBER','MERCHANT NAME','ORIGINAL GROSS AMT']].to_html()\n",
        "                        message = \"Prediction for whole file completed.\"\n",
        "                        processed_data = data\n",
        "                        button_name = \"Prediction for Whole File\"\n",
        "                    else:\n",
        "                        message = \"Please upload a valid file before calculating.\"\n",
        "                else:\n",
        "                    message = \"Please upload a file before calculating.\"\n",
        "            except ValueError:\n",
        "                message = \"Invalid input. Please enter valid numbers.\"\n",
        "\n",
        "        elif action == \"Predict for Card Number\":\n",
        "            try:\n",
        "                num1 = int(number1)\n",
        "                num2 = int(number2)\n",
        "                if uploaded_file:\n",
        "                    df = preprocess_data(uploaded_file)\n",
        "                    if not df.empty:\n",
        "                        data = card_predictions(df, num1, num2)\n",
        "                        table = data[['TRANS DATE','CARD NUMBER','MERCHANT NAME','ORIGINAL GROSS AMT']].to_html()\n",
        "                        processed_data = data\n",
        "                        message = \"Prediction for card number completed.\"\n",
        "                        button_name = \"Prediction for Card Number\"\n",
        "            except ValueError:\n",
        "                message = \"Invalid input. Please enter valid numbers.\"\n",
        "\n",
        "    return render_template_string(upload_form, message=message, table=table, button_name=button_name)\n",
        "\n",
        "@app.route(\"/download\", methods=[\"GET\"])\n",
        "def download():\n",
        "    global processed_data\n",
        "    if processed_data is not None:\n",
        "        output = io.StringIO()\n",
        "        processed_data.to_csv(output, index=False)\n",
        "        output.seek(0)\n",
        "\n",
        "        return send_file(\n",
        "            io.BytesIO(output.getvalue().encode()),\n",
        "            mimetype=\"text/csv\",\n",
        "            as_attachment=True,\n",
        "            download_name=\"predictions.csv\"\n",
        "        )\n",
        "    else:\n",
        "        return \"No data available to download.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAyNKLmLmYBy",
        "outputId": "7617eaba-48bd-45c3-e626-5f10374dd8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "Error: y contains previously unseen labels: [1077]\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [64]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [73]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [73]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Error: y contains previously unseen labels: [12]\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [1236]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "Error: y contains previously unseen labels: [1477]\n",
            "    CARD NUMBER TRANS DATE MERCHANT NAME DIREC  ORIGINAL GROSS AMT  Day  Year  \\\n",
            "3          4471 2017-01-02        others   NaN               16.44  NaN   NaN   \n",
            "4          5845 2018-12-15        others   NaN               16.78  NaN   NaN   \n",
            "5          8577 2018-12-14        others   NaN               23.45  NaN   NaN   \n",
            "6          4471 2019-02-26        others   NaN               16.40  NaN   NaN   \n",
            "7          5845 2020-08-11        others   NaN                7.67  NaN   NaN   \n",
            "8          8577 2021-02-03        others   NaN                7.59  NaN   NaN   \n",
            "9          4471 2020-01-04        others   NaN               27.06  NaN   NaN   \n",
            "10         5845 2020-08-20        others   NaN               18.31  NaN   NaN   \n",
            "11         8577 2022-04-21        others   NaN               39.15  NaN   NaN   \n",
            "12         4471 2022-12-21        others   NaN               35.76  NaN   NaN   \n",
            "13         5845 2024-08-31        others   NaN               23.33  NaN   NaN   \n",
            "14         8577 2025-04-08        others   NaN               51.09  NaN   NaN   \n",
            "15         4471 2026-02-25        others   NaN                9.51  NaN   NaN   \n",
            "16         5845 2024-11-23        others   NaN               33.36  NaN   NaN   \n",
            "17         8577 2028-01-15        others   NaN               55.05  NaN   NaN   \n",
            "\n",
            "    Month  \n",
            "3     NaN  \n",
            "4     NaN  \n",
            "5     NaN  \n",
            "6     NaN  \n",
            "7     NaN  \n",
            "8     NaN  \n",
            "9     NaN  \n",
            "10    NaN  \n",
            "11    NaN  \n",
            "12    NaN  \n",
            "13    NaN  \n",
            "14    NaN  \n",
            "15    NaN  \n",
            "16    NaN  \n",
            "17    NaN  \n"
          ]
        }
      ],
      "source": [
        "def mer_predict(data):\n",
        "    decode_mer = []\n",
        "\n",
        "    # Loop through each row in the data\n",
        "    for i in range(len(data)):\n",
        "        try:\n",
        "            # Predict for the specific row of data\n",
        "            row_data = data.iloc[[i]]  # Select row as DataFrame\n",
        "            mer = mer_model.predict(row_data)  # Predict for the specific row\n",
        "\n",
        "            # Try decoding the predicted value\n",
        "            try:\n",
        "                transformed_value = label_decode(mer)[0]  # Assuming label_decode returns an array\n",
        "                decode_mer.append(transformed_value)\n",
        "            except :\n",
        "                # If there's an error in decoding, append 'others'\n",
        "                decode_mer.append('others')\n",
        "\n",
        "        except :\n",
        "            # If there's an error in predicting for that row, append 'others'\n",
        "            decode_mer.append('others')\n",
        "\n",
        "    return decode_mer\n",
        "\n",
        "# Load and transform the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "df = df.head(3)\n",
        "#x, y, df = data_transform(df)\n",
        "\n",
        "# Predict and handle potential errors\n",
        "mer = all_predictions(df,5)\n",
        "print(mer)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}